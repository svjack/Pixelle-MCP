{
  "3": {
    "inputs": {
      "seed": 31,
      "steps": 8,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 1,
      "model": [
        "78",
        0
      ],
      "positive": [
        "94",
        0
      ],
      "negative": [
        "47",
        0
      ],
      "latent_image": [
        "98",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "K采样器"
    }
  },
  "18": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "106",
        0
      ]
    },
    "class_type": "VAEDecodeAudio",
    "_meta": {
      "title": "VAE解码（音频）"
    }
  },
  "47": {
    "inputs": {
      "conditioning": [
        "94",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "条件零化"
    }
  },
  "78": {
    "inputs": {
      "shift": 3,
      "model": [
        "104",
        0
      ]
    },
    "class_type": "ModelSamplingAuraFlow",
    "_meta": {
      "title": "采样算法（AuraFlow）"
    }
  },
  "94": {
    "inputs": {
      "tags": [
        "109",
        0
      ],
      "lyrics": [
        "110",
        0
      ],
      "seed": 31,
      "bpm": 190,
      "duration": [
        "113",
        0
      ],
      "timesignature": "4",
      "language": "en",
      "keyscale": "E minor",
      "generate_audio_codes": true,
      "cfg_scale": 2,
      "temperature": 0.85,
      "top_p": 0.9,
      "top_k": 0,
      "min_p": 0,
      "clip": [
        "105",
        0
      ]
    },
    "class_type": "TextEncodeAceStepAudio1.5",
    "_meta": {
      "title": "TextEncodeAceStepAudio1.5"
    }
  },
  "98": {
    "inputs": {
      "seconds": [
        "113",
        0
      ],
      "batch_size": 1
    },
    "class_type": "EmptyAceStep1.5LatentAudio",
    "_meta": {
      "title": "Empty Ace Step 1.5 Latent Audio"
    }
  },
  "104": {
    "inputs": {
      "unet_name": "acestep_v1.5_turbo.safetensors",
      "weight_dtype": "default"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "UNet加载器"
    }
  },
  "105": {
    "inputs": {
      "clip_name1": "qwen_0.6b_ace15.safetensors",
      "clip_name2": "qwen_4b_ace15.safetensors",
      "type": "ace",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "双CLIP加载器"
    }
  },
  "106": {
    "inputs": {
      "vae_name": "ace_1.5_vae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "加载VAE"
    }
  },
  "107": {
    "inputs": {
      "filename_prefix": "audio/ComfyUI",
      "quality": "V0",
      "audioUI": "",
      "audio": [
        "18",
        0
      ]
    },
    "class_type": "SaveAudioMP3",
    "_meta": {
      "title": "保存音频 (MP3)"
    }
  },
  "109": {
    "inputs": {
      "value": "Rock: A powerful, high-energy modern rock track with a raw and rebellious edge.\nDriven by distorted electric guitars, punchy live drums, and a strong bassline.\nThe song builds dynamically from tension to explosion, balancing aggression and emotion.\n"
    },
    "class_type": "PrimitiveStringMultiline",
    "_meta": {
      "title": "$music_style_prompt.value!:english prompt guide for global style of music"
    }
  },
  "110": {
    "inputs": {
      "value": "[Instrumental]"
    },
    "class_type": "PrimitiveStringMultiline",
    "_meta": {
      "title": "$Lyrics.value:Lyrics text"
    }
  },
  "111": {
    "inputs": {
      "value": 120
    },
    "class_type": "easy int",
    "_meta": {
      "title": "$length.value:length of music in second"
    }
  },
  "113": {
    "inputs": {
      "int": [
        "111",
        0
      ]
    },
    "class_type": "intToFloat _O",
    "_meta": {
      "title": "intToFloat _O"
    }
  },
  "114": {
    "inputs": {
      "text": "# ACE-Step 1.5 终极指南（必读）\n\n大家好，我是ACE-Step 的开发者，龚俊民。我将通过这篇教程，带你了解 ACE-Step 1.5 的设计哲学与使用方法。\n\n## 心智模型\n\n在开始之前，我们需要先建立正确的心智模型，以便做好预期管理。\n\n### 以人为中心的设计\n\n这个模型不是为**一键生成**而设计的，而是为**以人为中心的生成**而设计的。\n\n理解这个区别至关重要。\n\n### 什么是一键生成？\n\n你输入提示词，点击生成，听几个版本，选一个听起来不错的，拿来用。换一个人输入同样的提示词，大概率得到相似的结果。\n\n这种模式下，你和 AI 是**甲方与乙方**的关系。你带着明确的目的而来，脑海中已有一个模糊的预期，希望 AI 交付一个接近那个预期的成品。本质上，它和去 Google 搜索、去 Spotify 找歌没有太大区别——只是多了一点定制化。\n\nAI 是一种服务，而不是创意的启发者。\n\nSuno、Udio、MiniMax、Mureka——这些平台都是这样的设计初衷。它们可以把模型堆大，作为服务保障交付即可。你生成的音乐受他们的协议约束；你无法在本地运行，无法微调做个性化的探索；如果他们暗中改变模型或条款，你只能接受。\n\n### 什么是以人为中心的生成？\n\n如果我们弱化 AI 那一层，强化人那一层——让更多的人的意志、创意、灵感赋予 AI 生命——这就是以人为中心的生成。\n\n不同于一键生成的强目的性，以人为中心的生成更具备**玩**的性质。它更像一场交互的游戏，你和模型是**合作者**的关系。\n\n工作流是这样的：你先抛出一些灵感种子，得到几首歌，从中选择感兴趣的方向继续迭代——\n- 调整 prompt 重新生成\n- 用 **Cover** 保持结构、调整细节\n- 用 **Repaint** 做局部修改\n- 用 **Add Layer** 增减乐器层次\n\n这时，AI 对你而言不是服务者，而是**启发者**。\n\n### 这种设计需要满足什么条件？\n\n要让以人为中心的生成真正成立，模型必须满足几个关键条件：\n\n**首先，必须开源、本地可运行、可训练。**\n\n这不是技术洁癖，而是所有权的问题。当你使用闭源平台，你不拥有模型，你生成的作品也受制于他们的协议。版本更新、条款变更、服务下线——这些都不在你的掌控之中。\n\n而当模型开源且可本地运行，一切都不同了：**你永远拥有这个模型，你也永远拥有你和模型一起创造的所有产物。** 没有第三方协议的困扰，没有平台风险，你可以微调、可以魔改、可以基于它构建属于自己的创作系统，你的作品将永远属于你。他就像是你买的一个乐器，你可以随时随地使用它，也可以随时随地调整它。\n\n**其次，必须快。**\n\n人的时间是宝贵的，但更重要的是——**慢的生成会破坏心流**。\n\n以人为中心的工作流的核心是「试一试、听一听、再调整」的快速循环。如果每次生成都要等几分钟，你的灵感会在等待中消散，「玩」的体验会退化成「等」的煎熬。\n\n因此，我们专门为此优化了 ACE-Step：在保证质量的前提下，让生成速度足够快，快到能够承载一个流畅的人机对话节奏。\n\n### 有限游戏 vs 无限游戏\n\n一键生成是一场**有限游戏**——目标明确，结果导向，结束即终点。某种程度上，它冷酷地掏空了音乐产业，取代了许多人的工作。\n\n以人为中心的生成是一场**无限游戏**——因为乐趣藏在过程之中，而过程永无止境。\n\n我们的愿景是民主化 AI 音乐生成。让 ACE-Step 成为你口袋里的一个大玩具，让音乐回归 **Play** 本身——是融入创意的「玩」，而不只是点击播放的「Play」。\n\n---\n\n## 骑象人的隐喻\n\n> 推荐阅读：[The Complete Guide to Mastering Suno](https://www.notion.so/The-Complete-Guide-to-Mastering-Suno-Advanced-Strategies-for-Professional-Music-Generation-2d6ae744ebdf8024be42f6645f884221)——这篇博文教程能帮助你建立理解 AI 音乐的基础认知。\n\nAI 音乐生成，就像心理学中那个著名的**骑象人隐喻**。\n\n意识骑在潜意识上，人骑在大象上。你可以给出方向，但你无法让大象精准地、即时地执行你的每一个指令。它有自己的惯性，自己的脾气，自己的意志。\n\n这只大象，就是音乐生成模型。\n\n### 冰山模型\n\n音频和语义之间的联系，隐藏着一座冰山。\n\n我们能用语言描述的东西——风格、乐器、音色、情绪、场景、起承转合、歌词内容、演唱方式——这些都是我们熟悉的词汇，是我们能触及的部分。但它们加在一起，仍然只是音频这座冰山浮出海面的很小一角。\n\n最精准的控制是什么？是你拿着预期的音频输入，模型原封不动地返回输出。\n\n而只要你用的是文字描述、是参考、是提示——模型就一定有自己的发挥空间。这不是 bug，这是本质。\n\n### 大象是什么？\n\n这只大象是无数元素的融合体：数据的分布、模型的规模、算法的设计、标注的偏差、评估的偏差——**它是人类音乐历史与工程权衡的抽象结晶。**\n\n其中任意一个要素的偏差，都会导致它无法精准地反映你的品味和预期。\n\n当然，我们可以扩大数据规模、提升算法效率、提高标注精度、扩大模型容量、引入更专业的评估体系——这些都是作为模型开发者可以优化的方向。\n\n但即使有一天，技术上我们做到了「完美」，还有一个根本问题无法回避：**品味。**\n\n### 品味与期望\n\n品味因人而异。\n\n如果一个音乐生成模型试图取悦所有听众，它的输出将趋向于人类音乐历史的流行平均值——**这将平庸至极。**\n\n是人赋予声音以意义以情感以经历以生命，赋予它们文化符号的价值。是少数艺术家群体创造了独特的品味，然后带动普通人去消费、去追随，小众才变成了大众流行。这些少数派的先锋艺术家，便成为了传奇。\n\n所以，当你发现模型的输出「不对味」时，这有时可能不是模型的问题——**而是你的品味恰好不在那个「平均」上。** 这是好事。\n\n这意味着：**你需要学会引导这只大象，而不是期待它自动懂你。**\n\n---\n\n## 认识象群：模型架构与选择\n\n现在你已经理解了「大象」这个隐喻。但实际上——\n\n**这不是一只大象，而是一整个象群——大大小小的象，组成了一个家族。** 🐘🐘🐘🐘\n\n### 架构原理：两个大脑\n\nACE-Step 1.5 采用**混合架构**，有两个核心组件协同工作：\n\n```\n用户输入 → [5Hz LM] → 语义蓝图 → [DiT] → 音频\n              ↓\n         元数据推理\n         Caption 优化\n         结构规划\n```\n\n**5Hz LM（语言模型）—— 规划者（可选）**\n\nLM 是「全能规划器」，负责理解你的意图并制定计划：\n- 通过 **Chain-of-Thought** 推理音乐元数据（BPM、调性、时长等）\n- 优化和扩展你的 caption——这是对你意图的理解和补充\n- 生成**语义 codes**——隐式包含作曲旋律、配器编排、以及少许音色信息\n\nLM 学到的是训练数据中的**世界知识**。它是一个提升可用性、帮你快速生成原型的规划者。\n\n**但 LM 不是必须的。**\n\n如果你非常清楚自己想要什么，或者已经有了明确的规划目标——你完全可以不用 `thinking` 模式，跳过 LM 的规划步骤。\n\n比如在 **Cover 模式**中，你用参考音频来约束作曲、和弦、结构，直接让 DiT 去生成。这时，**你取代了 LM 的工作**——你自己成为了规划者。\n再比如，在 **Repaint 模式**中，你将参考音频作为上下文，限定音色、混音和细节，让 DiT 直接对局部进行调整。此时，DiT 更像是你的灵感激发伙伴，既可以帮助你进行创意脑暴，也可以用于修复局部的不和谐。\n\n**DiT（Diffusion Transformer）—— 执行者**\n\nDiT 是「音频工匠」，负责将规划变为现实：\n- 接收 LM 生成的语义 codes 和条件\n- 通过**扩散过程**从噪声中逐步「雕刻」出音频\n- 决定最终的音色、混音、细节\n\n**为什么这样设计？**\n\n传统方法让扩散模型直接从文本生成音频，但文本到音频的映射太模糊。ACE-Step 引入 LM 作为中间层：\n- LM 擅长理解语义、做规划\n- DiT 擅长生成高保真音频\n- 两者配合，各司其职\n\n### 选择规划者：LM 模型\n\nLM 有四种选择：**无 LM**（关闭 thinking 模式）、**0.6B**、**1.7B**、**4B**。\n\n它们的训练数据完全一致，区别纯粹在于**知识容量**：\n- 模型越大，世界知识越丰富\n- 模型越大，记忆能力越强（比如记住参考音频的旋律）\n- 模型越大，在长尾的风格或乐器中表现也相对更好\n\n| 选择 | 速度 | 世界知识 | 记忆能力 | 适用场景 |\n|------|:----:|:--------:|:--------:|----------|\n| 无 LM | ⚡⚡⚡⚡ | — | — | 你自己做规划（如 Cover 模式） |\n| `0.6B` | ⚡⚡⚡ | 基础 | 弱 | 低显存（< 8GB）、快速原型 |\n| `1.7B` | ⚡⚡ | 中等 | 中等 | **默认推荐** |\n| `4B` | ⚡ | 丰富 | 强 | 复杂任务、高质量生成 |\n\n**如何选择？**\n\n根据你的硬件条件：\n- **显存 < 8GB** → 无 LM 或 `0.6B`\n- **显存 8–16GB** → `1.7B`（默认）\n- **显存 > 16GB** → `1.7B` 或 `4B`\n\n### 选择执行者：DiT 模型\n\n有了规划方案，还需要选择执行者。DiT 是 ACE-Step 1.5 最核心的部分——它承载各种任务，决定如何解读 LM 生成的 codes。\n\n我们开源了 **4 个 Turbo 模型**、**1 个 SFT 模型**、**1 个 Base 模型**。\n\n#### Turbo 系列（推荐日常使用）\n\nTurbo 模型经过蒸馏训练，只需 8 步即可生成高质量音频。四个变体的核心差别在于**蒸馏时的 shift 超参配置**。\n\n**什么是 shift？**\n\nshift 决定了 DiT 去噪时的「注意力分配」：\n- **shift 越大** → 更多精力花在早期去噪（从纯噪声中建立大结构），**语义更强**，整体框架更清晰\n- **shift 越小** → 去噪步数分配更均匀，**细节更多**，但细节也可能是噪音\n\n简单理解：高 shift 像「先画轮廓再填细节」，低 shift 像「边画边修」。\n\n| 模型 | 蒸馏配置 | 特点 |\n|------|----------|------|\n| `turbo`（默认） | 在 shift 1, 2, 3 上联合蒸馏 | **创造性与语义兼顾最佳**，经过充分测试，推荐首选 |\n| `turbo-shift1` | 仅在 shift=1 上蒸馏 | 细节更丰富，但语义会弱一些 |\n| `turbo-shift3` | 仅在 shift=3 上蒸馏 | 音色更清晰丰富，但会显得「干」，配器偏极简 |\n| `turbo-continuous` | 实验性，支持 shift 1–5 连续调节 | 调参最灵活，但未经充分测试 |\n\n你可以根据目标音乐风格选择，也许你会发现自己对某个变体有偏好。**我们推荐从默认 turbo 开始**——它是最平衡、最经得起考验的选择。\n\n#### SFT 模型\n\n与 Turbo 相比，SFT 模型有两个显著特点：\n- **支持 CFG**（Classifier-Free Guidance），可以精细调节 prompt 遵循程度\n- **步数更多**（50 步），给模型更多时间「思考」\n\n代价是：步数多意味着误差累积，音质清晰度可能略逊于 Turbo。但它的**细节表现和语义解析会更好**。\n\n如果你不在乎推理时间，喜欢调 CFG 和步数，且偏好那种丰富的细节感——SFT 是一个好选择。LM 生成的 codes 同样可以作用于 SFT 模型。\n\n#### Base 模型\n\nBase 是**任务的集大成者**，比 SFT 和 Turbo 多出三个独占任务：\n\n| 任务 | 说明 |\n|------|------|\n| `extract` | 从混合轨道中抽取单个音轨（如分离人声） |\n| `lego` | 给已有轨道添加新轨道（如给吉他加鼓） |\n| `complete` | 给单轨添加混合伴奏（如给人声加吉他+鼓的伴奏） |\n\n此外，Base 的**可塑性最强**。如果你有大规模数据微调的需求，推荐从 Base 开始实验，训练专属你的 SFT 模型。\n\n#### 创建专属你的自定义模型\n\n除了官方模型，你还可以用 **LoRA 微调**创建专属你的自定义模型。\n\n我们会发布一个示例 LoRA 模型——用 20 多首「新年快乐」主题的歌曲训练，专门适合表达节日氛围。这只是一个起点。\n\n**自定义模型意味着什么？**\n\n你可以用专属的数据配方，重塑 DiT 的能力和偏好：\n- 喜欢某种特定的音色风格？用那类歌曲训练\n- 想让模型更擅长某种曲风？收集相关数据微调\n- 有自己独特的审美品味？把它「教」给模型\n\n这极大拓展了**定制化和可玩性**——用你的审美品味，训练出专属你的模型。\n\n> 关于 LoRA 训练的详细指南，请参阅 Gradio UI 中的「LoRA Training」标签页。\n\n#### DiT 选择总结\n\n| 模型 | 步数 | CFG | 速度 | 独占任务 | 推荐场景 |\n|------|:----:|:---:|:----:|----------|----------|\n| `turbo`（默认） | 8 | ❌ | ⚡⚡⚡ | — | 日常使用，快速迭代 |\n| `sft` | 50 | ✅ | ⚡ | — | 追求细节，喜欢调参 |\n| `base` | 50 | ✅ | ⚡ | extract, lego, complete | 特殊任务，大规模微调 |\n\n### 组合搭配\n\n默认配置是 **turbo + 1.7B LM**，适合大多数场景。\n\n| 需求 | 推荐组合 |\n|------|----------|\n| 最快速度 | `turbo` + 无 LM 或 `0.6B` |\n| 日常使用 | `turbo` + `1.7B`（默认） |\n| 追求细节 | `sft` + `1.7B` 或 `4B` |\n| 特殊任务 | `base`|\n| 大规模微调 | `base`|\n| 低显存（< 4GB） | `turbo` + 无 LM + CPU offload |\n\n### 下载模型\n\n```bash\n# 下载默认模型（turbo + 1.7B LM）\nuv run acestep-download\n\n# 下载全部模型\nuv run acestep-download --all\n\n# 下载特定模型\nuv run acestep-download --model acestep-v15-base\nuv run acestep-download --model acestep-5Hz-lm-0.6B\n\n# 查看可用模型\nuv run acestep-download --list\n```\n\n你需要把模型下载在一个`checkpoints`的文件夹中，方便识别。\n\n---\n\n## 引导大象：你能控制什么？\n\n现在你已经认识了这群大象，接下来让我们学习如何与它们沟通。\n\n每一次生成，结果都由三类因素共同决定：**输入控制**、**推理超参**和**随机因素**。\n\n### 一、输入控制：你想要什么？\n\n这是你与模型沟通「创意意图」的部分——你想生成什么样的音乐。\n\n| 类别 | 参数 | 作用 |\n|------|------|------|\n| **任务类型** | `task_type` | 决定生成模式：text2music、cover、repaint、lego、extract、complete |\n| **文本输入** | `caption` | 对音乐整体要素的描述：风格、乐器、情绪、氛围、音色、演唱者性别、起承转合等 |\n| | `lyrics` | 时序要素的描述：歌词内容、音乐结构演进、演唱者变化、人声/乐器演奏方式、开始/结束方式、发声方式等（纯音乐填 `[Instrumental]`） |\n| **音乐元数据** | `bpm` | 速度（30–300） |\n| | `keyscale` | 调性（如 C Major、Am） |\n| | `timesignature` | 拍号（4/4、3/4、6/8） |\n| | `vocal_language` | 人声语言 |\n| | `duration` | 目标时长（秒） |\n| **音频参考** | `reference_audio` | 音色或风格的全局参考（用于 cover、风格迁移） |\n| | `src_audio` | 源音频，用于非 text2music 任务（text2music 默认静音，无需输入） |\n| | `audio_codes` | Cover模式下输入模型的语义 codes（高级用法：复用 codes 生成变体、将歌曲转 codes 拓展衍生、拼接组合像 DJ 一样混搭） |\n| **区间控制** | `repainting_start/end` | 操作的时间区间（repaint 重绘区域 / lego 新增分轨区域） |\n\n---\n\n#### 关于 Caption：最重要的输入\n\n**Caption 是影响生成音乐最重要的因素。**\n\n它支持多种输入形式：简单的风格词、逗号分隔的 tags、复杂的自然语言描述。我们在训练中兼容了各种各样的格式，确保文本形式不会显著影响模型表现。\n\n**我们提供了至少 5 种方式帮你写好 caption：**\n\n1. **随机骰子** — 点击 UI 中的随机按钮，看看样例的 caption 是怎么写的。你可以把这个规范化的 caption 作为模板，让 LLM 帮你改写成想要的形式。\n\n2. **Format 自动改写** — 我们支持用 `format` 功能，把你手写的简单 caption 自动扩展成复杂描述。\n\n3. **CoT 重写** — 如果初始化了 LM，无论是否开启 `thinking` 模式，我们都支持通过 Chain-of-Thought 为你重写和扩展 caption（除非你在设置中主动关闭，或没有初始化 LM）。\n\n4. **音频转 Caption** — 我们的 LM 支持将你输入的音频转换为 caption。虽然精度有限，但模糊的方向是对的——足以作为起点。\n\n5. **Simple 模式** — 只需输入一句简单的歌曲描述，LM 会自动为你生成完整的 caption、lyrics 和 metas 样本——适合快速起步。\n\n无论哪种方式，它们都解决了一个现实问题：**作为普通人，我们的音乐词汇是贫瘠的。**\n\n如果你想让生成的音乐更有趣、更符合预期，**Prompting 始终是最优选项**——它带来的边际收益和惊喜最大。\n\n**Caption 写作的常用维度：**\n\n| 维度 | 示例 |\n|------|------|\n| **风格/流派** | pop, rock, jazz, electronic, hip-hop, R&B, folk, classical, lo-fi, synthwave |\n| **情绪/氛围** | melancholic, uplifting, energetic, dreamy, dark, nostalgic, euphoric, intimate |\n| **乐器** | acoustic guitar, piano, synth pads, 808 drums, strings, brass, electric bass |\n| **音色质感** | warm, bright, crisp, muddy, airy, punchy, lush, raw, polished |\n| **时代参考** | 80s synth-pop, 90s grunge, 2010s EDM, vintage soul, modern trap |\n| **制作风格** | lo-fi, high-fidelity, live recording, studio-polished, bedroom pop |\n| **人声特点** | female vocal, male vocal, breathy, powerful, falsetto, raspy, choir |\n| **速度/节奏** | slow tempo, mid-tempo, fast-paced, groovy, driving, laid-back |\n| **结构提示** | building intro, catchy chorus, dramatic bridge, fade-out ending |\n\n**一些实用原则：**\n\n1. **具体优于模糊** — 「sad piano ballad with female breathy vocal」比「a sad song」效果好。\n\n2. **组合多个维度** — 单一维度的描述会让模型有太多发挥空间，组合风格+情绪+乐器+音色能更精准地锚定你想要的方向。\n\n3. **善用参考** — 「in the style of 80s synthwave」或「reminiscent of Bon Iver」可以快速传达复杂的美学偏好。\n\n4. **质感词很有用** — warm, crisp, airy, punchy 这类形容词能影响混音和音色的倾向。\n\n5. **不必追求完美描述** — Caption 是起点而非终点。先写一个大致方向，生成后根据结果迭代调整。\n\n6. **描述粒度决定自由度** — 描述越省略，模型发挥空间越大，随机因素影响越多；描述越细致，模型越受约束。根据你的需求决定具体程度——想要惊喜就写少点，想要可控就写详细点。\n\n7. **避免冲突词汇** — 冲突的风格组合容易导致劣化输出。比如同时要「古典弦乐」和「硬核金属」，模型会尝试融合但通常不理想。尤其开启 `thinking` 模式时，LM 在 caption 泛化性上弱于 DiT。当你prompting不合理时，出来惊喜的概率更小。\n\n   **解决冲突的方法：**\n   - **重复强化** — 通过重复某些词来强化混搭风格中你更想要的元素\n   - **冲突变演变** — 把风格冲突转化为时间上的风格演变。比如：「开头是柔和的弦乐，中段变成噪杂动态的金属摇滚，结尾转为 hip-hop」——这样模型有明确的指引，知道如何处理不同风格，而不是把它们混成一团\n\n> 更多 prompting 技巧可参考：[The Complete Guide to Mastering Suno](https://www.notion.so/The-Complete-Guide-to-Mastering-Suno-Advanced-Strategies-for-Professional-Music-Generation-2d6ae744ebdf8024be42f6645f884221)——虽然是针对 Suno 的教程，但 prompting 的思路是通用的。\n\n---\n\n#### 关于 Lyrics：时序的脚本\n\n如果说 Caption 描述的是音乐的「整体画像」——风格、氛围、音色——那么 **Lyrics 就是音乐的「时间脚本」**，它控制着音乐如何随时间展开。\n\nLyrics 不仅仅是歌词内容。它承载着：\n- 歌词文本本身\n- **结构标记**（[Verse]、[Chorus]、[Bridge]...）\n- **演唱方式提示**（[raspy vocal]、[whispered]...）\n- **乐器段落**（[guitar solo]、[drum break]...）\n- **能量变化**（[building energy]、[explosive drop]...）\n\n**结构标记是关键**\n\n结构标记（Meta Tags）是 Lyrics 中最强大的工具。它们告诉模型：「这一段是什么，应该怎么表现。」\n\n**常用结构标记：**\n\n| 类别 | 标记 | 说明 |\n|------|------|------|\n| **基础结构** | `[Intro]` | 开场，建立氛围 |\n| | `[Verse]` / `[Verse 1]` | 主歌，叙事推进 |\n| | `[Pre-Chorus]` | 导歌，积蓄能量 |\n| | `[Chorus]` | 副歌，情感高潮 |\n| | `[Bridge]` | 桥段，转折或升华 |\n| | `[Outro]` | 结尾，收束 |\n| **动态段落** | `[Build]` | 能量逐渐攀升 |\n| | `[Drop]` | 电子乐的能量释放 |\n| | `[Breakdown]` | 配器减少，留白 |\n| **器乐段落** | `[Instrumental]` | 纯器乐，无人声 |\n| | `[Guitar Solo]` | 吉他独奏 |\n| | `[Piano Interlude]` | 钢琴间奏 |\n| **特殊标记** | `[Fade Out]` | 渐弱结束 |\n| | `[Silence]` | 静默 |\n\n**组合标记：适度使用**\n\n结构标记可以用 `-` 符号组合，实现更精细的控制：\n\n```\n[Chorus - anthemic]\n这是副歌的歌词\n梦想在燃烧\n\n[Bridge - whispered]\n轻轻地说出那些话\n```\n\n这比单独写 `[Chorus]` 效果好——你同时告诉了模型这段是什么（Chorus）、以及怎么唱（anthemic）。\n\n**⚠️ 注意：不要堆叠太多标记。**\n\n```\n❌ 不推荐：\n[Chorus - anthemic - stacked harmonies - high energy - powerful - epic]\n\n✅ 推荐：\n[Chorus - anthemic]\n```\n\n堆叠过多标记有两个风险：\n1. 模型可能把标记内容错误地当作歌词唱出来\n2. 过多指令会让模型困惑，效果反而变差\n\n**原则**：结构标记保持简洁，复杂的风格描述放在 Caption 中。\n\n**⚠️ 关键：保持 Caption 和 Lyrics 的一致性**\n\n**模型不擅长解决冲突。** 如果 Caption 和 Lyrics 中的描述相矛盾，模型会困惑，输出质量下降。\n\n```\n❌ 冲突示例：\nCaption: \"violin solo, classical, intimate chamber music\"\nLyrics: [Guitar Solo - electric - distorted]\n\n✅ 一致示例：\nCaption: \"violin solo, classical, intimate chamber music\"\nLyrics: [Violin Solo - expressive]\n```\n\n**检查清单**：\n- Caption 中的乐器 ↔ Lyrics 中的器乐段落标记\n- Caption 中的情绪 ↔ Lyrics 中的能量标记\n- Caption 中的人声描述 ↔ Lyrics 中的人声控制标记\n\n把 Caption 想象成「整体设定」，Lyrics 想象成「分镜脚本」——它们应该讲述同一个故事。\n\n**人声控制标记：**\n\n| 标记 | 效果 |\n|------|------|\n| `[raspy vocal]` | 沙哑、有质感的人声 |\n| `[whispered]` | 轻声细语 |\n| `[falsetto]` | 假声 |\n| `[powerful belting]` | 高亢有力的演唱 |\n| `[spoken word]` | 说唱/朗诵 |\n| `[harmonies]` | 和声层叠 |\n| `[call and response]` | 一呼一应 |\n| `[ad-lib]` | 即兴装饰音 |\n\n**能量与情绪标记：**\n\n| 标记 | 效果 |\n|------|------|\n| `[high energy]` | 高能量、激昂 |\n| `[low energy]` | 低能量、内敛 |\n| `[building energy]` | 能量递增 |\n| `[explosive]` | 爆发性能量 |\n| `[melancholic]` | 忧郁 |\n| `[euphoric]` | 欣快 |\n| `[dreamy]` | 梦幻 |\n| `[aggressive]` | 激进 |\n\n**歌词文本的写作技巧**\n\n**1. 控制音节数**\n\n每行 **6-10 个音节**通常效果最好。模型会将音节对齐到节拍上——如果一行 6 音节，下一行 14 音节，节奏会变得奇怪。\n\n```\n❌ 不好的例子：\n我站在窗前看着外面的世界一切都在改变（18 音节）\n你好（2 音节）\n\n✅ 好的例子：\n我站在窗前（5 音节）\n看着外面世界（6 音节）\n一切都在改变（6 音节）\n```\n\n**技巧**：同一位置的行（如每段的第一行）保持相近的音节数（±1-2 偏差）。\n\n**2. 用大小写控制强度**\n\n大写表示更强的演唱力度：\n\n```\n[Verse]\nwalking through the empty streets（正常力度）\n\n[Chorus]\nWE ARE THE CHAMPIONS!（高强度、呐喊）\n```\n\n**3. 用括号表示背景人声**\n\n```\n[Chorus]\nWe rise together (together)\nInto the light (into the light)\n```\n\n括号内的内容会被处理为背景人声或和声。\n\n**4. 延长元音**\n\n可以通过重复元音来延长音：\n\n```\nFeeeling so aliiive\n```\n\n但要谨慎使用——效果不稳定，有时会被忽略或发音错误。\n\n**5. 段落清晰分隔**\n\n每个段落之间用空行分隔：\n\n```\n[Verse 1]\n第一段的歌词\n继续第一段\n\n[Chorus]\n副歌的歌词\n副歌继续\n```\n\n**避免「AI 味」歌词**\n\n以下特征会让歌词显得机械、缺乏人味：\n\n| 红旗 🚩 | 说明 |\n|---------|------|\n| **形容词堆砌** | 「neon skies, electric hearts, endless dreams」——一段里塞满模糊意象 |\n| **押韵混乱** | 押韵模式不一致，或刻意凑韵导致语义断裂 |\n| **段落边界模糊** | 歌词内容跨越结构标记，Verse 的内容「流」进了 Chorus |\n| **没有呼吸感** | 每行太长，无法一口气唱完 |\n| **隐喻混用** | 第一段用水的意象，第二段突然变成火，第三段又是飞翔——听众无法锚定 |\n\n**隐喻纪律**：一首歌坚持一个核心隐喻，深入挖掘它的多个方面。比如选择「水」作为隐喻，就可以探索：爱如何像水一样绕过障碍、可以是细雨也可以是洪流、能倒映出对方的样子、无法握住却真实存在。一个意象，多个切面——这让歌词有凝聚力。\n\n**纯音乐的写法**\n\n如果生成纯器乐无人声：\n\n```\n[Instrumental]\n```\n\n或者用结构标记描述器乐的展开：\n\n```\n[Intro - ambient]\n\n[Main Theme - piano]\n\n[Climax - powerful]\n\n[Outro - fade out]\n```\n\n**完整示例**\n\n假设 Caption 是：`female vocal, piano ballad, emotional, intimate atmosphere, strings, building to powerful chorus`\n\n```\n[Intro - piano]\n\n[Verse 1]\n月光洒在窗台上\n我听见你的呼吸\n城市在远处沉睡\n只有我们还醒着\n\n[Pre-Chorus]\n这一刻如此安静\n却藏着汹涌的心\n\n[Chorus - powerful]\n让我们燃烧吧\n像夜空中的烟火\n短暂却绚烂\n这就是我们的时刻\n\n[Verse 2]\n时间在指尖流过\n我们抓不住什么\n但至少此刻拥有\n彼此眼中的火焰\n\n[Bridge - whispered]\n如果明天一切消散\n至少我们曾经闪耀\n\n[Final Chorus]\n让我们燃烧吧\n像夜空中的烟火\n短暂却绚烂\nTHIS IS OUR MOMENT!\n\n[Outro - fade out]\n```\n\n注意：这个示例中，Lyrics 的标记（piano、powerful、whispered）与 Caption 的描述（piano ballad、building to powerful chorus、intimate）保持一致，没有冲突。\n\n---\n\n#### 关于音乐元数据：可选的精细控制\n\n**大部分时候，你不需要手动设置元数据。**\n\n当你开启 `thinking` 模式（或启用 `use_cot_metas`），LM 会根据你的 Caption 和 Lyrics 自动推断合适的 BPM、调性、拍号等。这通常已经足够好了。\n\n但如果你有明确的想法，也可以手动控制它们：\n\n| 参数 | 控制范围 | 说明 |\n|------|----------|------|\n| `bpm` | 30–300 | 速度。常见分布：慢歌 60–80，中速 90–120，快歌 130–180 |\n| `keyscale` | 调性 | 如 `C Major`、`Am`、`F# Minor`。影响整体音高和情绪色彩 |\n| `timesignature` | 拍号 | `4/4`（最常见）、`3/4`（华尔兹）、`6/8`（摇摆感） |\n| `vocal_language` | 语言 | 人声的语言。LM 通常能根据歌词自动识别 |\n| `duration` | 秒 | 目标时长。实际生成可能略有偏差 |\n\n**理解控制的边界**\n\n这些参数是**引导**而非**精确指令**：\n\n- **BPM**：常见范围（60–180）控制效果较好；极端值（如 30 或 280）模型见过的数据少，可能不稳定\n- **调性**：常见调（C、G、D、Am、Em）效果稳定；冷门调可能被忽略或偏移\n- **拍号**：`4/4` 最可靠；`3/4`、`6/8` 通常 OK；复杂拍号（如 `5/4`、`7/8`）是高级玩法，效果因风格而异\n- **时长**：短歌（30–60s）和中等长度（2–4min）较稳定；超长生成可能出现重复或结构问题\n\n**模型的「参考」方式**\n\n模型并不是机械地执行 `bpm=120`，而是：\n1. 把 `120 BPM` 作为一个**锚点**\n2. 在这个锚点附近的分布中采样\n3. 最终结果可能是 118 或 122，而非精确的 120\n\n这就像告诉乐手「大概 120 的速度」——他们会在这个范围内自然演奏，而非死板地跟着节拍器。\n\n**什么时候需要手动设置？**\n\n| 场景 | 建议 |\n|------|------|\n| 日常生成 | 不用管，让 LM 自动推断 |\n| 有明确速度要求 | 手动设 `bpm` |\n| 做特定风格（如华尔兹） | 手动设 `timesignature=3/4` |\n| 要配合其他素材 | 手动设 `bpm` 和 `duration` |\n| 追求特定调性色彩 | 手动设 `keyscale` |\n\n**小贴士**：如果你手动设置了元数据，但生成结果明显不符合——检查一下是否和 Caption/Lyrics 有冲突。比如 Caption 写「slow ballad」但 `bpm=160`，模型会困惑。\n\n**推荐做法**：不要在 Caption 中写速度、BPM、调性等元数据信息。这些应该通过专门的元数据参数（`bpm`、`keyscale`、`timesignature` 等）来设置，而不是在 Caption 中描述。Caption 应该专注于风格、情绪、乐器、音色等音乐特征，而元数据信息交给对应的参数控制。\n\n---\n\n#### 关于音频控制：用声音控制声音\n\n**文本是降维抽象的，最好的控制还是用音频去控制。**\n\n用音频控制生成的方式有三种，它们各有不同的控制范围和用途：\n\n---\n\n##### 1. 参考音频（Reference Audio）：全局声学特征控制\n\n参考音频（`reference_audio`）用于控制生成音乐的**声学特征**——音色、混音风格、演奏风格等。它**平均了时间维度的信息**，是**作用全局**的。\n\n**参考音频控制什么？**\n\n参考音频主要控制生成音乐的**声学特征**，包括：\n- **音色质感**：人声的音色、乐器的音色\n- **混音风格**：空间感、动态范围、频率分布\n- **演奏风格**：演唱技巧、演奏技法、表达方式\n- **整体氛围**：通过参考音频传递的「感觉」\n\n**后台如何处理参考音频？**\n\n当你提供参考音频时，系统会进行以下处理：\n\n1. **音频预处理**：\n   - 加载音频文件，统一标准化为**立体声 48kHz** 格式\n   - 检测静音，如果音频完全静音则忽略\n   - 如果音频长度不足 30 秒，会重复填充到至少 30 秒\n   - 从前、中、后三个位置各随机选择 10 秒片段，拼接成 30 秒的参考片段\n\n2. **编码转换**：\n   - 使用 **VAE（变分自编码器）** 的 `tiled_encode` 方法将音频编码为**潜在表示（latents）**\n   - 这些 latents 包含了音频的声学特征信息，但去除了具体的旋律、节奏等结构信息\n   - 编码后的 latents 会作为条件输入到 DiT 的生成过程中，**平均时间维度信息，全局作用于整个生成过程**\n\n---\n\n##### 2. 源音频（Source Audio）：语义结构控制\n\n源音频（`src_audio`）用于 **Cover 任务**，进行**旋律结构控制**。它的原理是将你输入的源音频量化成语义结构化的信息。\n\n**源音频控制什么？**\n\n源音频会被转换为**语义结构化的信息**，包括：\n- **旋律**：音符的走向和音高\n- **节奏**：节拍、重音、律动\n- **和弦**：和声进行和变化\n- **配器**：乐器的编排和层次\n- **少许音色**：部分音色信息\n\n**你可以用它做什么？**\n\n1. **控制风格**：保持源音频的结构，改变风格和细节\n2. **迁移风格**：将源音频的结构应用到不同的风格中\n3. **Retake 抽卡**：生成相似结构但不同的变体，通过多次生成获得不同的演绎\n4. **控制影响程度**：通过 `audio_cover_strength` 参数（0.0–1.0）控制源音频的影响强度\n   - 强度越高，生成结果越严格遵循源音频的结构\n   - 强度越低，生成结果有更多自由发挥的空间\n\n**Cover 的高级用法**\n\n你可以用 Cover 去 **Remix 一首歌**，且支持更改 Caption 和 Lyrics：\n\n- **Remix 创作**：输入一首歌作为源音频，通过修改 Caption 和 Lyrics 来重新诠释它\n  - 改变风格：用不同的 Caption 描述（如从 pop 改为 rock）\n  - 改变歌词：用新的 Lyrics 重新填词，保持原有的旋律结构\n  - 改变情绪：通过 Caption 调整整体氛围（如从悲伤改为欢快）\n\n- **构建复杂的音乐结构**：根据你需要的结构影响程度来构建复杂的音乐旋律走向、层次和律动\n  - 通过 `audio_cover_strength` 精细控制结构遵循程度\n  - 结合 Caption 和 Lyrics 的修改，在保持核心结构的同时创造新的表达\n  - 可以生成多个版本，每个版本在结构、风格、歌词上有不同的侧重\n\n---\n\n##### 3. 基于源音频上下文的控制：局部补全与修改\n\n这是 **Repaint 任务**，基于源音频的上下文进行补全或修改。\n\n**Repaint 的原理**\n\nRepaint 基于**上下文补全**的原理：\n- 可以补全**开头**、**中间局部**、**结尾**，或**任意区域**\n- 操作范围：**3 秒到 90 秒**\n- 模型会参考源音频的上下文信息，在指定区间内进行生成\n\n**你可以用它做什么？**\n\n1. **局部修改**：修改指定区间的歌词、结构或内容\n2. **改歌词**：保持旋律和配器，只改变歌词内容\n3. **改结构**：在指定区间改变音乐结构（如将 Verse 改为 Chorus）\n4. **续写**：基于上下文续写开头或结尾\n5. **克隆音色**：基于上下文克隆源音频的音色特征\n\n**Repaint 的高级用法**\n\n你可以用 Repaint 实现更复杂的创作需求：\n\n- **无限时长生成**：\n  - 通过多次 Repaint 操作，可以不断续写音频，实现无限时长的生成\n  - 每次续写都会基于前一段的上下文，保持音乐的自然过渡和连贯性\n  - 可以分段生成，每段 3–90 秒，最终拼接成完整的作品\n\n- **智能音频缝合**：\n  - 将两个音频智能地组织缝合在一起\n  - 在第一个音频的结尾使用 Repaint 续写，让过渡自然衔接\n  - 或者用 Repaint 修改两个音频之间的连接部分，实现平滑过渡\n  - 模型会基于上下文自动处理节奏、和声、音色的衔接，让拼接后的音频听起来像一首完整的作品\n\n---\n\n##### 4. Base 模型的高级音频控制任务\n\n在 **Base 模型**中，我们还支持更多高级的音频控制任务：\n\n**Lego 任务**：基于已有轨道智能添加新轨道\n- 输入一个已有的音频轨道（如人声）\n- 模型会智能地添加新的轨道（如鼓、吉他、贝斯等）\n- 新轨道会与原有轨道在节奏、和声上协调配合\n\n**Complete 任务**：给单轨添加混合轨道\n- 输入一个单轨音频（如清唱人声）\n- 模型会生成完整的混合伴奏轨道\n- 生成的伴奏会与人声在风格、节奏、和声上匹配\n\n**这些高级上下文补全任务**极大拓展了控制方式，更智能地提供灵感和创意。\n\n---\n\n这些参数的组合，决定了你「想要什么」。后面我们会详细讲解输入控制的**原则**与**技巧**。\n\n### 二、推理超参：模型怎么生成？\n\n这是影响「生成过程行为」的部分——不改变你要什么，但改变模型怎么做。\n\n**DiT（扩散模型）超参：**\n\n| 参数 | 作用 | 默认值 | 调参建议 |\n|------|------|--------|----------|\n| `inference_steps` | 扩散步数 | 8 (turbo) | 越多越精细，但越慢。Turbo 用 8，Base 用 32–100 |\n| `guidance_scale` | CFG 强度 | 7.0 | 越高越遵循 prompt，但可能过拟合。仅 Base 模型有效 |\n| `use_adg` | 自适应双重引导 | False | 开启后动态调整 CFG，仅 Base 模型 |\n| `cfg_interval_start/end` | CFG 生效区间 | 0.0–1.0 | 控制在哪个阶段应用 CFG |\n| `shift` | 时间步偏移 | 1.0 | 调整去噪轨迹，影响生成风格 |\n| `infer_method` | 推理方法 | \"ode\" | `ode` 确定性，`sde` 引入随机性 |\n| `timesteps` | 自定义时间步 | None | 高级用法，覆盖 steps 和 shift |\n| `audio_cover_strength` | 参考音频/codes 的影响强度 | 1.0 | 0.0–1.0，越高越接近参考，越低越自由发挥 |\n\n**5Hz LM（语言模型）超参：**\n\n| 参数 | 作用 | 默认值 | 调参建议 |\n|------|------|--------|----------|\n| `thinking` | 启用 CoT 推理 | True | 开启让 LM 推理元数据和 codes |\n| `lm_temperature` | 采样温度 | 0.85 | 越高越随机/创意，越低越保守/确定 |\n| `lm_cfg_scale` | LM 的 CFG 强度 | 2.0 | 越高越遵循正向 prompt |\n| `lm_top_k` | Top-K 采样 | 0 | 0 表示禁用，限制候选词数量 |\n| `lm_top_p` | Top-P 采样 | 0.9 | 核采样，限制累积概率 |\n| `lm_negative_prompt` | 负面提示 | \"NO USER INPUT\" | 告诉 LM 不要生成什么 |\n| `use_cot_metas` | CoT 推理元数据 | True | 让 LM 自动推断 BPM、调性等 |\n| `use_cot_caption` | CoT 重写 caption | True | 让 LM 优化你的描述 |\n| `use_cot_language` | CoT 检测语言 | True | 让 LM 自动识别人声语言 |\n| `use_constrained_decoding` | 约束解码 | True | 确保输出格式正确 |\n\n这些参数的组合，决定了模型「怎么做」。\n\n**关于调参的说明**\n\n需要强调的是，**调参因素和随机因素有时候影响相当**。当你调整某个参数时，可能很难判断是参数本身的影响，还是随机性带来的变化。\n\n因此，**推荐固定随机因素去调参**——通过设置固定的 `seed` 值，确保每次生成都从相同的初始噪声开始，这样你才能准确感受到参数对生成音频的真实影响。否则，参数变化的效果可能会被随机性掩盖，让你误判参数的作用。\n\n### 三、随机因素：不确定性的来源\n\n即使所有输入和超参完全相同，两次生成的结果也可能不同。这是因为：\n\n**1. DiT 的初始噪声**\n- 扩散模型从随机噪声开始逐步去噪\n- `seed` 参数控制这个初始噪声\n- 不同的 seed → 不同的起点 → 不同的终点\n\n**2. LM 的采样随机性**\n- 当 `lm_temperature > 0`，采样过程本身带有随机性\n- 同样的 prompt，每次采样可能选择不同的 token\n\n**3. `infer_method = \"sde\"` 时的额外噪声**\n- SDE 方法在去噪过程中注入额外随机性\n\n---\n\n#### 随机因素的利弊\n\n随机性是一把双刃剑。\n\n**随机性的好处**：\n- **探索创作空间**：同样的输入可以产生不同的变体，给你更多选择\n- **发现意外惊喜**：有时候随机性会带来你意想不到的优秀结果\n- **避免重复**：每次生成都有所不同，不会陷入单一模式的循环\n\n**随机性的挑战**：\n- **结果不可控**：你无法精确预测生成结果，可能多次生成都不满意\n- **难以复现**：即使输入完全相同，也很难复现某个特定的好结果\n- **调参困难**：当你调整参数时，很难判断是参数的影响还是随机性的变化\n- **筛选成本**：需要生成多个版本才能找到满意的，增加了时间成本\n\n#### 用怎样的心态面对随机因素？\n\n**1. 接受不确定性**\n- 随机性是 AI 音乐生成的本质特征，不是 bug，而是 feature\n- 不要期望每次生成都完美，把随机性当作探索的工具\n\n**2. 拥抱探索过程**\n- 把生成过程看作「抽卡」或「挖宝」——多试几次，总能找到惊喜\n- 享受发现意外好结果的过程，而不是执着于一次成功\n\n**3. 善用固定 seed**\n- 当你想要**理解参数影响**时，固定 `seed` 来消除随机性干扰\n- 当你想要**探索创作空间**时，让 `seed` 随机变化\n\n**4. 批量生成 + 智能筛选**\n- 不要依赖单次生成，而是批量生成多个版本\n- 利用自动打分机制进行初步筛选，提高效率\n\n#### 我们的解决方案：大 Batch + 自动打分\n\n因为我们推理速度极快，如果你的显卡显存足够，你可以通过**大 batch 去探索随机空间**：\n\n- **批量生成**：一次生成多个版本（如 batch_size=2,4,8），快速探索随机空间\n- **自动打分机制**：我们提供自动打分机制，可以帮助你初步筛选，做 **test time scaling**\n\n**自动打分机制**\n\n我们提供了多种打分指标，其中**我最喜欢的是 DiT Lyrics Alignment Score**：\n\n- **DiT Lyrics Alignment Score**：这个分数隐式地影响了歌词的准确性\n  - 它评估生成音频中歌词与音频的对齐程度\n  - 分数越高，说明歌词在音频中的位置越准确，演唱与歌词的匹配度越好\n  - 这对于有歌词的音乐生成特别重要，可以帮助你筛选出歌词准确性更高的版本\n\n- **其他打分指标**：还包括其他质量评估指标，可以从多个维度评估生成结果\n\n**工作流程建议**：\n\n1. **批量生成**：设置较大的 `batch_size`（如 2、4、8），一次生成多个版本\n2. **开启 AutoGen**：启用自动生成功能，让系统在后台持续生成新的批次\n   - **AutoGen 的机制**：AutoGen 会在你查看当前批次结果时，自动在后台使用相同的参数（但使用随机 seed）生成下一批次\n   - 这样你可以持续探索随机空间，而不需要手动点击生成按钮\n   - 每个新批次都会使用新的随机 seed，确保结果的多样性\n3. **自动打分**：开启自动打分功能，让系统自动为每个版本打分\n4. **初步筛选**：根据 DiT Lyrics Alignment Score 等指标，筛选出分数较高的版本\n5. **人工精挑**：在筛选出的版本中，人工选择最符合你需求的最终版本\n\n这样既能充分利用随机性探索创作空间，又能通过自动化工具提高效率，避免在大量生成结果中盲目寻找。AutoGen 让你可以「边听边生成」，在浏览当前结果的同时，下一批次已经在后台准备好了。\n\n---\n\n## 结语\n\n这篇教程目前涵盖了 ACE-Step 1.5 的核心概念和使用方法：\n\n- **心智模型**：理解以人为中心的生成设计哲学\n- **模型架构**：认识 LM 和 DiT 的协同工作方式\n- **输入控制**：掌握文本（Caption、Lyrics、元数据）和音频（参考音频、源音频）的控制方法\n- **推理超参**：了解影响生成过程的参数\n- **随机因素**：学会利用随机性探索创作空间，并通过大 Batch + AutoGen + 自动打分提高效率\n\n这只是一个开始。还有很多内容我们想分享给你：\n\n- 更多 Prompting 技巧和实战案例\n- 不同任务类型的详细使用指南\n- 高级玩法和创意工作流\n- 常见问题与解决方案\n- 性能优化建议\n\n**这篇教程会持续更新和完善。** 如果你在使用过程中有任何问题或建议，欢迎反馈。让我们一起让 ACE-Step 成为你口袋里的创作伙伴。\n"
    },
    "class_type": "Text _O",
    "_meta": {
      "title": "MCP"
    }
  }
}